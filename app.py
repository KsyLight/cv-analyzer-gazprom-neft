import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import mplcyberpunk

plt.style.use('cyberpunk')

from utils.cv_reader import read_resume_from_file, preprocess_text
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.constants import competency_list, profession_matrix, profession_names

# â”€â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ ĞĞ»ÑŒÑĞ½ÑĞ° Ğ˜Ğ˜",
    layout="wide",
    initial_sidebar_state="collapsed"
)
st.title("ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ ĞĞ»ÑŒÑĞ½ÑĞ° Ğ˜Ğ˜")

# â”€â”€â”€ Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/errors.log",
    level=logging.ERROR,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s"
)

# â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(
        repo_id, token=st.secrets["HUGGINGFACE_TOKEN"]
    )
    model = AutoModelForSequenceClassification.from_pretrained(
        repo_id, token=st.secrets["HUGGINGFACE_TOKEN"]
    )
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

def predict_competencies(text: str):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    preds = (probs > 0.5).astype(int)
    return preds, probs

# â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ€ĞµĞ·ÑĞ¼Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded_file = st.file_uploader(
    "ğŸ“¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ (PDF, DOCX, TXT)",
    type=["pdf", "docx", "txt"]
)

if uploaded_file:
    os.makedirs("temp", exist_ok=True)
    tmp_file_path = os.path.join("temp", uploaded_file.name)

    with open(tmp_file_path, "wb") as f:
        f.write(uploaded_file.read())

    try:
        with st.spinner("â³ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°..."):
            base_text = read_resume_from_file(tmp_file_path)
            if not base_text:
                st.error("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ·ÑĞ¼Ğµ.")
                st.stop()

            gh_links = extract_github_links_from_text(base_text)
            github_text_raw = ""
            if gh_links:
                st.markdown("ğŸ”— **GitHubâ€‘ÑÑÑ‹Ğ»ĞºĞ¸:**")
                for link in gh_links:
                    st.markdown(f"- [{link}]({link})")
                    try:
                        github_text_raw += " " + collect_github_text(link)
                    except Exception as e:
                        st.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ {link}")
                        logging.error(f"GitHub fetch error ({link}): {e}")
            else:
                st.info("GitHubâ€‘ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹.")

            st.session_state["github_text_raw"] = github_text_raw
            full_text = preprocess_text(base_text + " " + github_text_raw)

        with st.spinner("ğŸ¤– ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹..."):
            pred_vector, prob_vector = predict_competencies(full_text)

        tab1, tab2, tab3 = st.tabs(["ĞĞ¿Ñ€Ğ¾Ñ", "ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸", "Ğ ĞµĞ·ÑĞ¼Ğµ"])

        # â”€â”€â”€ Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ° 1: ĞĞ¿Ñ€Ğ¾Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab1:
            st.subheader("Ğ’Ğ°Ñˆ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸ÑĞ¼ (0â€“3):")
            user_grades = []
            col1, col2 = st.columns(2)
            for i, comp in enumerate(competency_list):
                default = 1 if pred_vector[i] else 0
                with col1 if (i % 2 == 0) else col2:
                    grade = st.radio(
                        comp,
                        [0, 1, 2, 3],
                        index=default,
                        horizontal=True,
                        key=f"grade_{i}"
                    )
                    user_grades.append(grade)
            st.session_state.user_grades = user_grades
            st.success("âœ… Ğ“Ñ€ĞµĞ¹Ğ´Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹! ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ 'ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸'")

        # â”€â”€â”€ Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ° 2: ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab2:
            if "user_grades" not in st.session_state:
                st.warning("âš ï¸ Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ³Ñ€ĞµĞ¹Ğ´Ñ‹ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ 'ĞĞ¿Ñ€Ğ¾Ñ'")
                st.stop()

            user_vector = np.array(st.session_state.user_grades)
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ³Ñ€ĞµĞ¹Ğ´Ğ¾Ğ² ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑÑ‚Ñ€Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ
            if user_vector.shape[0] != profession_matrix.shape[0]:
                st.error("âš ï¸ Ğ§Ğ¸ÑĞ»Ğ¾ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ½Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹.")
                st.stop()

            col1, col2 = st.columns(2)

            # ---- Ğ›ĞµĞ²Ñ‹Ğ¹ ÑÑ‚Ğ¾Ğ»Ğ±ĞµÑ†: Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹ ----
            with col1:
                st.markdown("### Ğ’Ğ°ÑˆĞ¸ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ñ€ĞµĞ¹Ğ´Ñ‹:")
                st.markdown("""
                    <div style="border:1px solid #ddd; border-radius:8px; padding:10px; width:60%;">
                        <p style="margin:0; line-height:1.4em;">
                            <b>ğŸŸ© â€” Ğ³Ñ€ĞµĞ¹Ğ´Â 3</b> (Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ)<br>
                            <b>ğŸŸ¨ â€” Ğ³Ñ€ĞµĞ¹Ğ´Â 2</b> (ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ)<br>
                            <b>ğŸŸ¦ â€” Ğ³Ñ€ĞµĞ¹Ğ´Â 1</b> (Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ)<br>
                            <b>â¬œï¸ â€” Ğ³Ñ€ĞµĞ¹Ğ´Â 0</b> (Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚)
                        </p>
                    </div>
                """, unsafe_allow_html=True)

                sorted_comps = sorted(
                    zip(competency_list, user_vector),
                    key=lambda x: -x[1]
                )
                for comp, grade in sorted_comps:
                    emoji = {3: "ğŸŸ©", 2: "ğŸŸ¨", 1: "ğŸŸ¦", 0: "â¬œï¸"}[grade]
                    st.markdown(f"{emoji} **{comp}** â€” Ğ³Ñ€ĞµĞ¹Ğ´: **{grade}**")

            # ---- ĞŸÑ€Ğ°Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ¾Ğ»Ğ±ĞµÑ†: Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ ----
            with col2:
                # 1) Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ relative ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ
                percentages = []
                for i, prof in enumerate(profession_names):
                    required = profession_matrix[:, i]
                    total = np.count_nonzero(required)
                    if total > 0:
                        matched = np.count_nonzero((user_vector >= required) & (required > 0))
                        pct = matched / total * 100
                    else:
                        pct = 0.0
                    percentages.append((prof, pct))
                sorted_percentages = sorted(percentages, key=lambda x: x[1], reverse=True)

                # === ĞšÑ€ÑƒĞ³Ğ¾Ğ²Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ° ===
                fig, ax = plt.subplots(figsize=(6, 6))
                fig.patch.set_facecolor('#0d1117')
                ax.set_facecolor('#0d1117')

                labels = [p for p, _ in sorted_percentages]
                values = [v for _, v in sorted_percentages]
                palette = sns.color_palette("pastel", len(labels))
                wedges, texts, autotexts = ax.pie(
                    values,
                    labels=labels,
                    autopct="%1.1f%%",
                    startangle=90,
                    colors=palette,
                    wedgeprops={'edgecolor':'#0d1117','linewidth':1}
                )
                for txt in texts + autotexts:
                    txt.set_color('white')
                    txt.set_fontsize(11)

                ax.axis('equal')
                ax.set_title("ĞÑ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸ÑĞ¼", color='white', pad=20)
                mplcyberpunk.add_glow_effects()
                st.pyplot(fig)

                # === Ğ¡Ñ‚Ğ¾Ğ»Ğ±Ñ‡Ğ°Ñ‚Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ° ===
                st.markdown("### ĞĞ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸ÑĞ¼")
                fig_bar, ax_bar = plt.subplots(figsize=(8, 4))
                fig_bar.patch.set_facecolor('#0d1117')
                ax_bar.set_facecolor('#0d1117')

                bars = ax_bar.barh(
                    labels,
                    values,
                    color=sns.color_palette("dark", len(labels)),
                    edgecolor='white',
                    linewidth=0.8
                )
                ax_bar.set_xlim(0, 100)
                ax_bar.invert_yaxis()
                ax_bar.set_xlabel("ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ", color='white')
                ax_bar.set_title("Ğ’Ğ°ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸", color='white', pad=15)
                ax_bar.grid(axis='x', linestyle='--', alpha=0.3)

                # Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ½Ğ°Ğ´ Ğ±Ğ°Ñ€Ğ°Ğ¼Ğ¸
                for bar in bars:
                    w = bar.get_width()
                    ax_bar.text(
                        w + 1,
                        bar.get_y() + bar.get_height()/2,
                        f"{w:.1f}%",
                        va='center',
                        color='white',
                        fontsize=10
                    )

                mplcyberpunk.add_glow_effects()
                st.pyplot(fig_bar)

                # --- ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¹ (Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹) ---
                st.markdown("### ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¹")
                # ... Ğ²Ğ°Ñˆ ĞºĞ¾Ğ´ Ğ¿Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ ...

        # â”€â”€â”€ Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ° 3: Ğ ĞµĞ·ÑĞ¼Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab3:
            st.markdown("### Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ·ÑĞ¼Ğµ")
            with st.expander("ğŸ“ Ğ¢ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ° Ñ€ĞµĞ·ÑĞ¼Ğµ"):
                st.text(base_text)

            github_text_final = st.session_state.get("github_text_raw", "")
            if github_text_final.strip():
                with st.expander("ğŸ§‘â€ğŸ’» Ğ¢ĞµĞºÑÑ‚, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ GitHub"):
                    st.text(github_text_final)
            else:
                st.info("GitHubâ€‘ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ¸Ğ»Ğ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ.")

    except Exception as e:
        st.error("ğŸš« ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ».")
        logging.error(f"ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}", exc_info=True)

    finally:
        # Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        try:
            os.remove(tmp_file_path)
        except OSError:
            pass