import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import mplcyberpunk

plt.style.use('cyberpunk')

from utils.cv_reader import read_resume_from_file, preprocess_text
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.constants import (
    competency_list,
    profession_matrix,
    profession_names,
    recommendations,
)

# â”€â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ ĞĞ»ÑŒÑĞ½ÑĞ° Ğ˜Ğ˜",
    layout="wide",
    initial_sidebar_state="collapsed"
)
st.title("ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ ĞĞ»ÑŒÑĞ½ÑĞ° Ğ˜Ğ˜")

# â”€â”€â”€ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GitHub-Ñ‚ĞµĞºÑÑ‚Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "github_text_raw" not in st.session_state:
    st.session_state["github_text_raw"] = ""

# â”€â”€â”€ Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/errors.log",
    level=logging.ERROR,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s"
)

# â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(
        repo_id,
        token=st.secrets["HUGGINGFACE_TOKEN"]
    )
    model = AutoModelForSequenceClassification.from_pretrained(
        repo_id,
        token=st.secrets["HUGGINGFACE_TOKEN"]
    )
    model.eval()
    return tokenizer, model

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
tokenizer, model = load_model()

def predict_competencies(text: str):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    preds = (probs > 0.46269254347612143).astype(int)
    return preds, probs

# â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ€ĞµĞ·ÑĞ¼Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded_file = st.file_uploader(
    "ğŸ“¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ (PDF, DOCX, TXT)",
    type=["pdf", "docx", "txt"]
)

if uploaded_file:
    os.makedirs("temp", exist_ok=True)
    tmp_file_path = os.path.join("temp", uploaded_file.name)
    with open(tmp_file_path, "wb") as f:
        f.write(uploaded_file.read())

    try:
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°
        with st.spinner("â³ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°..."):
            base_text = read_resume_from_file(tmp_file_path)
            if not base_text:
                st.error("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ·ÑĞ¼Ğµ.")
                st.stop()

            # GitHub-ÑÑÑ‹Ğ»ĞºĞ¸
            gh_links = extract_github_links_from_text(base_text)
            github_text_raw = ""
            if gh_links:
                st.markdown("ğŸ”— **GitHubâ€‘ÑÑÑ‹Ğ»ĞºĞ¸:**")
                for link in gh_links:
                    st.markdown(f"- [{link}]({link})")
                    try:
                        github_text_raw += "\n" + collect_github_text(link)
                    except Exception as e:
                        st.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ {link}")
                        logging.error(f"GitHub fetch error ({link}): {e}")
            else:
                st.info("GitHubâ€‘ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹.")
            st.session_state["github_text_raw"] = github_text_raw
            full_text = preprocess_text(base_text + " " + github_text_raw)

        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹
        with st.spinner("ğŸ¤– ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹..."):
            pred_vector, prob_vector = predict_competencies(full_text)

        # â”€â”€â”€ Ğ’ĞºĞ»Ğ°Ğ´ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tab1, tab2, tab3, tab4 = st.tabs([
            "ĞĞ¿Ñ€Ğ¾Ñ", "ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸", "Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸", "Ğ ĞµĞ·ÑĞ¼Ğµ"
        ])

        # â”€â”€â”€ Ğ¢Ğ°Ğ± 1: ĞĞ¿Ñ€Ğ¾Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab1:
            st.subheader("Ğ’Ğ°Ñˆ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸ÑĞ¼ (0â€“3):")
            user_grades = []
            col1, col2 = st.columns(2)
            for i, comp in enumerate(competency_list):
                default = 1 if pred_vector[i] else 0
                with col1 if i % 2 == 0 else col2:
                    grade = st.radio(
                        comp,
                        [0, 1, 2, 3],
                        index=default,
                        horizontal=True,
                        key=f"grade_{i}"
                    )
                    user_grades.append(grade)
            st.session_state.user_grades = user_grades
            st.success("âœ… Ğ“Ñ€ĞµĞ¹Ğ´Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹! ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ 'ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸'")

        # â”€â”€â”€ Ğ¢Ğ°Ğ± 2: ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab2:
            if "user_grades" not in st.session_state:
                st.warning("âš ï¸ Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ³Ñ€ĞµĞ¹Ğ´Ñ‹ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ 'ĞĞ¿Ñ€Ğ¾Ñ'")
                st.stop()
            user_vector = np.array(st.session_state.user_grades)
            if user_vector.shape[0] != profession_matrix.shape[0]:
                st.error("âš ï¸ Ğ§Ğ¸ÑĞ»Ğ¾ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ½Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹.")
                st.stop()
            col1, col2 = st.columns(2)

            # Ğ›ĞµĞ²Ñ‹Ğ¹ ÑÑ‚Ğ¾Ğ»Ğ±ĞµÑ†
            with col1:
                st.markdown("### Ğ’Ğ°ÑˆĞ¸ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ñ€ĞµĞ¹Ğ´Ñ‹:")
                st.markdown(
                    "<div style='border:1px solid #ddd; border-radius:8px; padding:10px; margin-bottom:10px; width:60%; background:#1a1a1a;'>"
                    "<p style='margin:0; padding-left:12px; color:white; line-height:1.4em;'>"
                    "<strong style='color:#4caf50;'>ğŸŸ© â€” Ğ³Ñ€ĞµĞ¹Ğ´ 3</strong><br>"
                    "<strong style='color:#ffeb3b;'>ğŸŸ¨ â€” Ğ³Ñ€ĞµĞ¹Ğ´ 2</strong><br>"
                    "<strong style='color:#2196f3;'>ğŸŸ¦ â€” Ğ³Ñ€ĞµĞ¹Ğ´ 1</strong><br>"
                    "<strong style='color:#ffffff;'>â¬œï¸ â€” Ğ³Ñ€ĞµĞ¹Ğ´ 0</strong>"
                    "</p></div>", unsafe_allow_html=True
                )
                for comp, grade in sorted(zip(competency_list, user_vector), key=lambda x: -x[1]):
                    emoji = {3: "ğŸŸ©", 2: "ğŸŸ¨", 1: "ğŸŸ¦", 0: "â¬œï¸"}[grade]
                    color = "#4caf50" if grade==3 else "#ffeb3b" if grade==2 else "#2196f3" if grade==1 else "#ffffff"
                    st.markdown(
                        f"<div style='margin-left:20px; color:white;'>{emoji} <span style='color:{color};'><strong>{comp}</strong></span> â€” Ğ³Ñ€ĞµĞ¹Ğ´: <strong>{grade}</strong></div>",
                        unsafe_allow_html=True
                    )

            # ĞŸÑ€Ğ°Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ¾Ğ»Ğ±ĞµÑ†: Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°
            with col2:
                percentages = []
                for i, prof in enumerate(profession_names):
                    req = profession_matrix[:, i]
                    tot = np.count_nonzero(req)
                    match = np.count_nonzero((user_vector >= req) & (req > 0))
                    pct = match / tot * 100 if tot else 0.0
                    percentages.append((prof, pct))
                sorted_pct = sorted(percentages, key=lambda x: x[1], reverse=True)
                labels = [p for p, _ in sorted_pct]
                values = [v for _, v in sorted_pct]
                colors = sns.color_palette("dark", len(labels))

                fig, ax = plt.subplots(figsize=(6, 6))
                fig.patch.set_facecolor('#0d1117'); ax.set_facecolor('#0d1117')
                wedges, texts, atxts = ax.pie(
                    values, labels=labels, autopct="%1.1f%%", startangle=90,
                    colors=colors, wedgeprops={'edgecolor':'white','linewidth':0.8}
                )
                for t in texts + atxts:
                    t.set_color('white'); t.set_fontsize(10)
                ax.axis('equal'); mplcyberpunk.add_glow_effects()
                st.markdown("### ĞÑ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸ÑĞ¼")
                st.pyplot(fig)

                fig_bar, ax_bar = plt.subplots(figsize=(8, 4))
                fig_bar.patch.set_facecolor('#0d1117'); ax_bar.set_facecolor('#0d1117')
                bars = ax_bar.barh(labels, values, color=colors, edgecolor='white', linewidth=0.8)
                ax_bar.set_xlim(0, 100); ax_bar.invert_yaxis()
                ax_bar.set_xlabel("ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ", color='white')
                ax_bar.grid(axis='x', linestyle='--', alpha=0.3)
                for bar in bars:
                    w = bar.get_width()
                    ax_bar.text(w+1, bar.get_y()+bar.get_height()/2, f"{w:.1f}%", va='center', color='white', fontsize=10)
                mplcyberpunk.add_glow_effects()
                st.markdown("### ĞĞ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸ÑĞ¼")
                st.pyplot(fig_bar)

                # Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¹
                st.markdown("### ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¹")
                descriptions = {
                    "ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Data scientist, ML engineer)":
                        "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜. â€¢ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ML Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ â€¢ Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ â€¢ Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ â€¢ Ğ’ĞµĞ´Ñ‘Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ",
                    "ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ² Ğ˜Ğ˜ (Manager in AI)":
                        "Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ¼, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ€Ğ¾ĞºĞ¸ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹. ĞÑ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ², Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ„Ğ¸Ğ´Ğ±ĞµĞºĞ°.",
                    "Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ğ˜Ğ˜ (Technical analyst in AI)":
                        "Ğ¡Ğ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°ĞºĞ°Ğ·Ñ‡Ğ¸ĞºĞ° Ğ¸ ML-ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ Ğ¢Ğ— Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.",
                    "Ğ˜Ğ½Ğ¶ĞµĞ½ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Data engineer)":
                        "Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚, Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚, Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚. ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                }
                prof_name_mapping = {
                    "ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…": "ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Data scientist, ML engineer)",
                    "ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ² Ğ˜Ğ˜": "ĞœĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ² Ğ˜Ğ˜ (Manager in AI)",
                    "Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ğ˜Ğ˜": "Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ² Ğ˜Ğ˜ (Technical analyst in AI)",
                    "Ğ˜Ğ½Ğ¶ĞµĞ½ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…": "Ğ˜Ğ½Ğ¶ĞµĞ½ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Data engineer)"
                }
                table_rows = ""
                for prof,_ in sorted_pct:
                    full_name = prof_name_mapping.get(prof, prof)
                    desc = descriptions.get(full_name, "â€”")
                    parts = desc.split(" â€¢ ")
                    if len(parts) > 1:
                        intro = parts[0].strip()
                        items = parts[1:]
                        desc_html = f"<p style='margin:0 0 4px 0;'>{intro}</p><ul style='margin:0; padding-left:20px;'>" + "".join(f"<li style='margin-bottom:2px;'>{it.strip()}</li>" for it in items) + "</ul>"
                    else:
                        desc_html = f"<p style='margin:0;'>{desc}</p>"
                    table_rows += f"<tr><td style='border:1px solid #444; padding:8px; color:white; vertical-align:top;'>{full_name}</td><td style='border:1px solid #444; padding:8px; color:white; vertical-align:top;'>{desc_html}</td></tr>"
                table_html = f"<table style='width:100%; border-collapse:collapse;'><thead><tr style='background-color:#1f1f1f;'><th style='border:1px solid #555; padding:8px; color:white; text-align:left;'>ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ</th><th style='border:1px solid #555; padding:8px; color:white; text-align:left;'>ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ</th></tr></thead><tbody>{table_rows}</tbody></table>"
                st.markdown(table_html, unsafe_allow_html=True)

        # â”€â”€â”€ Ğ¢Ğ°Ğ± 3: Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab3:
            st.subheader("Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸")
            prof_choice = st.selectbox("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ", profession_names)
            idx = profession_names.index(prof_choice)
            req_vec = profession_matrix[:, idx]
            user_vec = np.array(st.session_state.user_grades)

            sort_asc = st.checkbox("Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ³Ñ€ĞµĞ¹Ğ´Ñƒ (Ğ¼ĞµĞ½ÑŒÑˆĞµ â†’ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ)", value=True)
            weak = [i for i,(u,r) in enumerate(zip(user_vec, req_vec)) if u < r]
            weak_sorted = sorted(weak, key=lambda i: user_vec[i], reverse=not sort_asc)

            emoji_map = {3: "ğŸŸ©", 2: "ğŸŸ¨", 1: "ğŸŸ¦", 0: "â¬œï¸"}
            if not recommendations:
                st.info("Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¿ÑƒÑÑ‚. Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² utils/constants.py")
            elif weak_sorted:
                for i in weak_sorted:
                    comp = competency_list[i]
                    g = user_vec[i]
                    req = req_vec[i]
                    emoji = emoji_map.get(g, "â¬œï¸")
                    color = "#4caf50" if g==3 else "#ffeb3b" if g==2 else "#2196f3" if g==1 else "#ffffff"
                    st.markdown(f"<span style='color:{color};'>{emoji} <strong>{comp}</strong></span>: Ğ²Ğ°Ñˆ Ğ³Ñ€ĞµĞ¹Ğ´ {g}, Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ {req}", unsafe_allow_html=True)
                    recs = recommendations.get(comp, [])
                    if recs:
                        for url in recs:
                            st.markdown(f"- [{url}]({url})")
                    else:
                        st.markdown("- Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ½ĞµÑ‚ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸")
            else:
                st.success("Ğ’ÑĞµ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼!")

        # â”€â”€â”€ Ğ¢Ğ°Ğ± 4: Ğ ĞµĞ·ÑĞ¼Ğµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab4:
            st.markdown("### Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ·ÑĞ¼Ğµ")
            # Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ·ÑĞ¼Ğµ
            with st.expander("ğŸ“ Ğ¢ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ° Ñ€ĞµĞ·ÑĞ¼Ğµ"):
                st.text(base_text)

            # Ğ¢ĞµĞºÑÑ‚ Ñ GitHub
            st.expander("ğŸ§‘â€ğŸ’» Ğ¢ĞµĞºÑÑ‚, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ GitHub")
            with st.expander("ğŸ§‘â€ğŸ’» Ğ¢ĞµĞºÑÑ‚, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ GitHub", expanded=True):
                github_text_final = st.session_state.get("github_text_raw", "")
                if github_text_final:
                    st.text_area("GitHub-Ñ‚ĞµĞºÑÑ‚", github_text_final, height=300)
                else:
                    if gh_links:
                        st.warning("âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub.")
                    else:
                        st.info("GitHubâ€‘ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹.")
    except Exception as e:
        st.error("ğŸš« ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ».")
        logging.error(f"ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}", exc_info=True)
    finally:
        try:
            os.remove(tmp_file_path)
        except OSError:
            pass