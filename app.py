import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login
import torch
import numpy as np
import os
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import mplcyberpunk

plt.style.use('cyberpunk')

from utils.cv_reader import read_resume_from_file, preprocess_text
from utils.github_reader import extract_github_links_from_text, collect_github_text
from utils.constants import competency_list, profession_matrix, profession_names

# ‚îÄ‚îÄ‚îÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –ø–æ –º–∞—Ç—Ä–∏—Ü–µ –ê–ª—å—è–Ω—Å–∞ –ò–ò",
    layout="wide",
    initial_sidebar_state="collapsed"
)
st.title("–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –ø–æ –º–∞—Ç—Ä–∏—Ü–µ –ê–ª—å—è–Ω—Å–∞ –ò–ò")

# ‚îÄ‚îÄ‚îÄ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
os.makedirs("logs", exist_ok=True)
logging.basicConfig(
    filename="logs/errors.log",
    level=logging.ERROR,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s"
)

# ‚îÄ‚îÄ‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@st.cache_resource
def load_model():
    login(token=st.secrets["HUGGINGFACE_TOKEN"])
    repo_id = "KsyLight/resume-ai-competency-model"
    tokenizer = AutoTokenizer.from_pretrained(
        repo_id, token=st.secrets["HUGGINGFACE_TOKEN"]
    )
    model = AutoModelForSequenceClassification.from_pretrained(
        repo_id, token=st.secrets["HUGGINGFACE_TOKEN"]
    )
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

def predict_competencies(text: str):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()
    preds = (probs > 0.5).astype(int)
    return preds, probs

# ‚îÄ‚îÄ‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—é–º–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
uploaded_file = st.file_uploader(
    "üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ (PDF, DOCX, TXT)",
    type=["pdf", "docx", "txt"]
)

if uploaded_file:
    os.makedirs("temp", exist_ok=True)
    tmp_file_path = os.path.join("temp", uploaded_file.name)

    with open(tmp_file_path, "wb") as f:
        f.write(uploaded_file.read())

    try:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        with st.spinner("‚è≥ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."):
            base_text = read_resume_from_file(tmp_file_path)
            if not base_text:
                st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ.")
                st.stop()

            # GitHub
            gh_links = extract_github_links_from_text(base_text)
            github_text_raw = ""
            if gh_links:
                st.markdown("üîó **GitHub‚Äë—Å—Å—ã–ª–∫–∏:**")
                for link in gh_links:
                    st.markdown(f"- [{link}]({link})")
                    try:
                        github_text_raw += " " + collect_github_text(link)
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {link}")
                        logging.error(f"GitHub fetch error ({link}): {e}")
            else:
                st.info("GitHub‚Äë—Å—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

            st.session_state["github_text_raw"] = github_text_raw
            full_text = preprocess_text(base_text + " " + github_text_raw)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
        with st.spinner("ü§ñ –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π..."):
            pred_vector, prob_vector = predict_competencies(full_text)

        # –¢–∞–±—ã
        tab1, tab2, tab3 = st.tabs(["–û–ø—Ä–æ—Å", "–ü—Ä–æ—Ñ–µ—Å—Å–∏–∏", "–†–µ–∑—é–º–µ"])

        # ‚îÄ‚îÄ‚îÄ –¢–∞–± 1: –û–ø—Ä–æ—Å ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        with tab1:
            st.subheader("–í–∞—à —É—Ä–æ–≤–µ–Ω—å –≤–ª–∞–¥–µ–Ω–∏—è –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º (0‚Äì3):")
            user_grades = []
            col1, col2 = st.columns(2)
            for i, comp in enumerate(competency_list):
                default = 1 if pred_vector[i] else 0
                with col1 if (i % 2 == 0) else col2:
                    grade = st.radio(
                        comp,
                        [0, 1, 2, 3],
                        index=default,
                        horizontal=True,
                        key=f"grade_{i}"
                    )
                    user_grades.append(grade)
            st.session_state.user_grades = user_grades
            st.success("‚úÖ –ì—Ä–µ–π–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫—É '–ü—Ä–æ—Ñ–µ—Å—Å–∏–∏'")

        # ‚îÄ‚îÄ‚îÄ –¢–∞–± 2: –ü—Ä–æ—Ñ–µ—Å—Å–∏–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        with tab2:
            if "user_grades" not in st.session_state:
                st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≥—Ä–µ–π–¥—ã –≤–æ –≤–∫–ª–∞–¥–∫–µ '–û–ø—Ä–æ—Å'")
                st.stop()

            user_vector = np.array(st.session_state.user_grades)
            if user_vector.shape[0] != profession_matrix.shape[0]:
                st.error("‚ö†Ô∏è –ß–∏—Å–ª–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –º–∞—Ç—Ä–∏—Ü—ã.")
                st.stop()

            col1, col2 = st.columns(2)

            # ‚Äî‚Äî‚Äî –õ–µ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ –≥—Ä–µ–π–¥—ã —Å –æ—Ç—Å—Ç—É–ø–æ–º ‚Äî‚Äî‚Äî
            with col1:
                st.markdown("### –í–∞—à–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ –≥—Ä–µ–π–¥—ã:")
                st.markdown("""
                    <div style="
                        border:1px solid #ddd;
                        border-radius:8px;
                        padding:10px;
                        margin-bottom:10px;
                        width:60%;
                    ">
                      <p style="margin:0; line-height:1.4em; padding-left:10px;">
                        <b>üü© ‚Äî –≥—Ä–µ–π–¥¬†3</b> (–≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å)<br>
                        <b>üü® ‚Äî –≥—Ä–µ–π–¥¬†2</b> (—É–≤–µ—Ä–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å)<br>
                        <b>üü¶ ‚Äî –≥—Ä–µ–π–¥¬†1</b> (–Ω–∞—á–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å)<br>
                        <b>‚¨úÔ∏è ‚Äî –≥—Ä–µ–π–¥¬†0</b> (–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)
                      </p>
                    </div>
                """, unsafe_allow_html=True)

                sorted_comps = sorted(
                    zip(competency_list, user_vector),
                    key=lambda x: -x[1]
                )
                for comp, grade in sorted_comps:
                    emoji = {3: "üü©", 2: "üü®", 1: "üü¶", 0: "‚¨úÔ∏è"}[grade]
                    st.markdown(
                        f"<div style='margin-left:20px;'>{emoji} **{comp}** ‚Äî –≥—Ä–µ–π–¥: **{grade}**</div>",
                        unsafe_allow_html=True
                    )

            # ‚Äî‚Äî‚Äî –ü—Ä–∞–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü: –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–π ‚Äî‚Äî‚Äî
            with col2:
                # –†–∞—Å—á—ë—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                percentages = []
                for i, prof in enumerate(profession_names):
                    required = profession_matrix[:, i]
                    total = np.count_nonzero(required)
                    matched = np.count_nonzero((user_vector >= required) & (required > 0))
                    pct = matched / total * 100 if total else 0.0
                    percentages.append((prof, pct))

                sorted_percentages = sorted(percentages, key=lambda x: x[1], reverse=True)
                labels = [p for p, _ in sorted_percentages]
                values = [v for _, v in sorted_percentages]

                # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
                fig, ax = plt.subplots(figsize=(6, 6))
                fig.patch.set_facecolor('#0d1117')
                ax.set_facecolor('#0d1117')
                palette = sns.color_palette("pastel", len(labels))
                wedges, texts, autotexts = ax.pie(
                    values,
                    labels=labels,
                    autopct="%1.1f%%",
                    startangle=90,
                    colors=palette,
                    wedgeprops={'edgecolor':'#0d1117','linewidth':1}
                )
                for txt in texts + autotexts:
                    txt.set_color('white')
                    txt.set_fontsize(11)
                ax.axis('equal')
                mplcyberpunk.add_glow_effects()
                st.markdown("### –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º")
                st.pyplot(fig)

                # –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
                fig_bar, ax_bar = plt.subplots(figsize=(8, 4))
                fig_bar.patch.set_facecolor('#0d1117')
                ax_bar.set_facecolor('#0d1117')
                bars = ax_bar.barh(
                    labels,
                    values,
                    color=sns.color_palette("dark", len(labels)),
                    edgecolor='white',
                    linewidth=0.8
                )
                ax_bar.set_xlim(0, 100)
                ax_bar.invert_yaxis()
                ax_bar.set_xlabel("–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è", color='white')
                ax_bar.grid(axis='x', linestyle='--', alpha=0.3)
                for bar in bars:
                    w = bar.get_width()
                    ax_bar.text(
                        w + 1,
                        bar.get_y() + bar.get_height() / 2,
                        f"{w:.1f}%",
                        va='center',
                        color='white',
                        fontsize=10
                    )
                mplcyberpunk.add_glow_effects()
                st.markdown("### –ê–±—Å–æ–ª—é—Ç–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º")
                st.pyplot(fig_bar)

                # –ë–ª–æ–∫ –æ–ø–∏—Å–∞–Ω–∏–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–π
                st.markdown("### –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π")
                descriptions = {
                    "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö (Data scientist, ML engineer)": """–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏–∏, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Ö –∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò. –°–æ–≤–º–µ—Å—Ç–Ω–æ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞–º–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫.
‚Ä¢ –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–µ—Ç–æ–¥ ML –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –µ–≥–æ –∫ –∑–∞–¥–∞—á–µ  
‚Ä¢ –†–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏  
‚Ä¢ –°—Ç—Ä–æ–∏—Ç –ø–∞–π–ø–ª–∞–π–Ω  
‚Ä¢ –í–µ–¥—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é""",
                    "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò (Manager in AI)": """–†—É–∫–æ–≤–æ–¥–∏—Ç –ø—Ä–æ–µ–∫—Ç–æ–º, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å—Ä–æ–∫–∏ –∏ —Ä–µ—Å—É—Ä—Å—ã. –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è –≤ –ø—Ä–æ–¥—É–∫—Ç–∏–≤, –º–æ–∂–µ—Ç —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∏–¥–±–µ–∫–∞.""",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò (Technical analyst in AI)": """–°–≤—è–∑—ã–≤–∞–µ—Ç –∑–∞–∫–∞–∑—á–∏–∫–∞ –∏ ML-–∫–æ–º–∞–Ω–¥—É. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã, –≥–æ—Ç–æ–≤–∏—Ç –¢–ó –∏ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –æ—Ü–µ–Ω–∫–µ —Ä–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏.""",
                    "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö (Data engineer)": """–ì–æ—Ç–æ–≤–∏—Ç –¥–∞–Ω–Ω—ã–µ: —Å–æ–±–∏—Ä–∞–µ—Ç, –æ—á–∏—â–∞–µ—Ç, –ø–µ—Ä–µ–¥–∞—ë—Ç. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏ –ø–∞–π–ø–ª–∞–π–Ω—ã –¥–∞–Ω–Ω—ã—Ö."""
                }
                prof_name_mapping = {
                    "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö": "–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö (Data scientist, ML engineer)",
                    "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò": "–ú–µ–Ω–µ–¥–∂–µ—Ä –≤ –ò–ò (Manager in AI)",
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –ò–ò (Technical analyst in AI)",
                    "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö": "–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö (Data engineer)"
                }
                for prof, _ in sorted_percentages:
                    full_name = prof_name_mapping.get(prof, prof)
                    desc = descriptions.get(full_name, "‚Äî")
                    st.markdown(f"""
                        <div style="
                            border:1px solid #ddd;
                            border-radius:8px;
                            padding:10px;
                            margin-bottom:10px;
                        ">
                            <h4 style="margin:0 0 5px 0;">{full_name}</h4>
                            <p style="margin:0;">{desc}</p>
                        </div>
                    """, unsafe_allow_html=True)

        # ‚îÄ‚îÄ‚îÄ –¢–∞–± 3: –†–µ–∑—é–º–µ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        with tab3:
            st.markdown("### –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ")
            with st.expander("üìù –¢–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ —Ä–µ–∑—é–º–µ"):
                st.text(base_text)

            github_text_final = st.session_state.get("github_text_raw", "")
            if github_text_final.strip():
                with st.expander("üßë‚Äçüíª –¢–µ–∫—Å—Ç, —Å–æ–±—Ä–∞–Ω–Ω—ã–π —Å GitHub"):
                    st.text(github_text_final)
            else:
                st.info("GitHub‚Äë—Å—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.")

    except Exception as e:
        st.error("üö´ –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª.")
        logging.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
    finally:
        try:
            os.remove(tmp_file_path)
        except OSError:
            pass